{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting CIK for every company in the s&p 500 from wikipedia\n",
    "\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "cik_df = pd.read_html(wiki_url, header=0, index_col=0)[0]\n",
    "cik_list = list(cik_df['CIK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty dataframe to append all other dataframes with 8-k links\n",
    "doc_df = pd.DataFrame()\n",
    "\n",
    "for cik in cik_list:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # defining endpoint and parameters for every company in the s&p \n",
    "        url = 'https://www.sec.gov/cgi-bin/browse-edgar'\n",
    "        params = {'action': 'getcompany', \n",
    "          'CIK': cik, \n",
    "          'type': '8-K', \n",
    "          'output':'xml', \n",
    "          'dateb': '20200430',\n",
    "          'datea': '20150501',\n",
    "          'start': '',\n",
    "          'count': '100'}\n",
    "\n",
    "        # getting response from EDGAR database\n",
    "        sec_response = requests.get(url=url, params=params)\n",
    "\n",
    "        # creating soup to parse xml\n",
    "        soup = BeautifulSoup(sec_response.content, 'xml')\n",
    "\n",
    "        # getting link to 8-k document\n",
    "        urls = soup.findAll('filingHREF')\n",
    "        html_list = []\n",
    "\n",
    "        # html version of links\n",
    "        for url in urls:\n",
    "            url = url.string\n",
    "\n",
    "            if url.split('.')[len(url.split('.')) - 1] == 'htm':\n",
    "                txt_link = url + 'l'\n",
    "                html_list.append(txt_link)\n",
    "        \n",
    "        html_list = pd.Series(html_list).astype(str)\n",
    "        \n",
    "        # list of links\n",
    "        doc_list = html_list.str.replace('-index.html', '.txt').values.tolist()\n",
    "\n",
    "        # creating dataframe to append the link of each company\n",
    "        \n",
    "        df = pd.DataFrame({'cik': [cik]*len(doc_list),\n",
    "                   'txt_link': doc_list})\n",
    "\n",
    "        doc_df = doc_df.append(df)\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df.to_csv('D:/Python/lstm-stock-predictor/data/raw/8-k_links.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
